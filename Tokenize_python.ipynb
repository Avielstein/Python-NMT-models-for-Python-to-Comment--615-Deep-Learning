{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import import_ipynb\n",
    "import Extract_Python_Pairs as EPP\n",
    "#https://docs.python.org/3/library/tokenize.html\n",
    "import tokenize\n",
    "import io\n",
    "import numpy as np\n",
    "from tqdm import tqdm #inline progress bar (quality of life)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15320/16115540 [00:00<01:45, 153191.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed in 227 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16115540/16115540 [00:36<00:00, 444800.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 2508330\n"
     ]
    }
   ],
   "source": [
    "#get the code and comment snippets from the Extract Python Pairs file\n",
    "code_snippets, comments = EPP.get_python_pairs(EPP.get_all_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ENDMARKER = 0\n",
    "NAME = 1\n",
    "NUMBER = 2\n",
    "STRING = 3\n",
    "NEWLINE = 4\n",
    "INDENT = 5\n",
    "DEDENT = 6\n",
    "LPAR = 7\n",
    "RPAR = 8\n",
    "LSQB = 9\n",
    "RSQB = 10\n",
    "COLON = 11\n",
    "COMMA = 12\n",
    "SEMI = 13\n",
    "PLUS = 14\n",
    "MINUS = 15\n",
    "STAR = 16\n",
    "SLASH = 17\n",
    "VBAR = 18\n",
    "AMPER = 19\n",
    "LESS = 20\n",
    "GREATER = 21\n",
    "EQUAL = 22\n",
    "DOT = 23\n",
    "PERCENT = 24\n",
    "LBRACE = 25\n",
    "RBRACE = 26\n",
    "EQEQUAL = 27\n",
    "NOTEQUAL = 28\n",
    "LESSEQUAL = 29\n",
    "GREATEREQUAL = 30\n",
    "TILDE = 31\n",
    "CIRCUMFLEX = 32\n",
    "LEFTSHIFT = 33\n",
    "RIGHTSHIFT = 34\n",
    "DOUBLESTAR = 35\n",
    "PLUSEQUAL = 36\n",
    "MINEQUAL = 37\n",
    "STAREQUAL = 38\n",
    "SLASHEQUAL = 39\n",
    "PERCENTEQUAL = 40\n",
    "AMPEREQUAL = 41\n",
    "VBAREQUAL = 42\n",
    "CIRCUMFLEXEQUAL = 43\n",
    "LEFTSHIFTEQUAL = 44\n",
    "RIGHTSHIFTEQUAL = 45\n",
    "DOUBLESTAREQUAL = 46\n",
    "DOUBLESLASH = 47\n",
    "DOUBLESLASHEQUAL = 48\n",
    "AT = 49\n",
    "ATEQUAL = 50\n",
    "RARROW = 51\n",
    "ELLIPSIS = 52\n",
    "COLONEQUAL = 53\n",
    "OP = 54\n",
    "AWAIT = 55\n",
    "ASYNC = 56\n",
    "TYPE_IGNORE = 57\n",
    "TYPE_COMMENT = 58\n",
    "# These aren't used by the C tokenizer but are needed for tokenize.py\n",
    "ERRORTOKEN = 59\n",
    "COMMENT = 60\n",
    "NL = 61\n",
    "ENCODING = 62\n",
    "N_TOKENS = 63\n",
    "# Special definitions for cooperation with parser\n",
    "NT_OFFSET = 256\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "  def __call__(mcs, *args, **kwargs):\n",
      "    \n",
      "    if kwargs:\n",
      "      return type.__call__(mcs, *args, **kwargs)\n",
      "    else:\n",
      "      instances = mcs._instances\n",
      "      key = (mcs,) + tuple(args)\n",
      "      try:\n",
      "        return instances[key]\n",
      "      except KeyError:\n",
      "        # No cache entry for key exists, create a new one.\n",
      "        return instances.setdefault(key, type.__call__(mcs, *args))\n",
      "      except TypeError:\n",
      "        # An object in args cannot be hashed, always return\n",
      "        # a new instance.\n",
      "        return type.__call__(mcs, *args)\n",
      "\n",
      "\n",
      "\n",
      "527\n",
      "\n",
      "Tokenized:\n",
      "['  ', 'def', '__call__', '(', 'mcs', ',', '*', 'args', ',', '**', 'kwargs', ')', ':', '\\n', '    ', 'if', 'kwargs', ':', '\\n', '      ', 'return', 'type', '.', '__call__', '(', 'mcs', ',', '*', 'args', ',', '**', 'kwargs', ')', '\\n', '', 'else', ':', '\\n', '      ', 'instances', '=', 'mcs', '.', '_instances', '\\n', 'key', '=', '(', 'mcs', ',', ')', '+', 'tuple', '(', 'args', ')', '\\n', 'try', ':', '\\n', '        ', 'return', 'instances', '[', 'key', ']', '\\n', '', 'except', 'KeyError', ':', '\\n', '        ', 'return', 'instances', '.', 'setdefault', '(', 'key', ',', 'type', '.', '__call__', '(', 'mcs', ',', '*', 'args', ')', ')', '\\n', '', 'except', 'TypeError', ':', '\\n', '        ', 'return', 'type', '.', '__call__', '(', 'mcs', ',', '*', 'args', ')', '\\n', '', '', '', '']\n",
      "number of tokens: 112\n"
     ]
    }
   ],
   "source": [
    "#https://docs.python.org/3/library/tokenize.html\n",
    "def tokenize_python(code_snippet):\n",
    "    tokens = tokenize.tokenize(io.BytesIO(code_snippet.encode('utf-8')).readline)\n",
    "    parsed = []\n",
    "    for token in tokens:\n",
    "        if token.type not in [0,57,58,59,60,61,62,63,256]:\n",
    "            #print('----')\n",
    "            #print(token)\n",
    "            parsed.append(token.string)\n",
    "    return parsed\n",
    "\n",
    "#EXAMPLE\n",
    "print('Original:')\n",
    "print(code_snippets[0])\n",
    "print(len(code_snippets[0]))\n",
    "print()\n",
    "print('Tokenized:')\n",
    "print(tokenize_python(code_snippets[0]))\n",
    "print('number of tokens:',len(tokenize_python(code_snippets[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small_code = code_snippets[:10000]\n",
    "#small_comment =  comments[:10000]\n",
    "small_code = code_snippets\n",
    "small_comment =  comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2508330/2508330 [16:36<00:00, 2517.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of errors: 74483 or approixmatiely  2.9694258729911933 % of samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Source_Sequences = [] #eng_sent = []\n",
    "Target_Sequences = [] #fra_sent = []\n",
    "Source_Tokens = set() #eng_chars = set()\n",
    "Target_Tokens = set() #fra_chars = set()\n",
    "\n",
    "\n",
    "errors = 0\n",
    "for i in tqdm(range(len(small_code))):\n",
    "    try:\n",
    "        Eng = ''.join(ch for ch in small_comment[i] if ch.isalnum() or ch ==' ').lower()\n",
    "        Eng_toks = nltk.word_tokenize(Eng)\n",
    "        Target_Sequences.append(Eng_toks)\n",
    "\n",
    "        python_toks = tokenize_python(small_code[i])\n",
    "        Source_Sequences.append(python_toks)\n",
    "\n",
    "\n",
    "        for nodeType in python_toks:\n",
    "            if (nodeType not in Source_Tokens):\n",
    "                Source_Tokens.add(nodeType)\n",
    "\n",
    "\n",
    "        for word in Eng_toks:\n",
    "            if (word not in Target_Tokens):\n",
    "                Target_Tokens.add(word)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        errors+=1\n",
    "\n",
    "print('number of errors:',errors,'or approixmatiely ',(errors/len(small_code))*100,'% of samples')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source_Tokens = sorted(list(Source_Tokens))\n",
    "Target_Tokens = sorted(list(Target_Tokens))\n",
    "nb_samples= len(Source_Sequences) #for all\n",
    "\n",
    "# dictionary to index each english character - key is index and value is english character\n",
    "source_index_to_token_dict = {}\n",
    "\n",
    "# dictionary to get english character given its index - key is english character and value is index\n",
    "source_token_to_index_dict = {}\n",
    "\n",
    "for k, v in enumerate(Source_Tokens):\n",
    "    source_index_to_token_dict[k] = v\n",
    "    source_token_to_index_dict[v] = k\n",
    "\n",
    "# dictionary to index each french character - key is index and value is french character\n",
    "target_index_to_token_dict = {}\n",
    "\n",
    "# dictionary to get french character given its index - key is french character and value is index\n",
    "target_token_to_index_dict = {}\n",
    "\n",
    "for k, v in enumerate(Target_Tokens):\n",
    "    target_index_to_token_dict[k] = v\n",
    "    target_token_to_index_dict[v] = k\n",
    "    \n",
    "    \n",
    "max_len_source_seq = max([len(line) for line in Source_Sequences])\n",
    "max_len_target_seq = max([len(line) for line in Target_Sequences])\n",
    "\n",
    "print('max length source: ',max_len_source_seq)\n",
    "print('source vocab: ',len(Source_Tokens))\n",
    "print('max length target: ',max_len_target_seq)\n",
    "print('target vocab: ',len(Target_Tokens))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
