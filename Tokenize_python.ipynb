{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Extract_Python_Pairs.ipynb\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import import_ipynb\n",
    "import Extract_Python_Pairs as EPP\n",
    "#https://docs.python.org/3/library/tokenize.html\n",
    "import tokenize\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 65309/16115540 [00:00<00:24, 653089.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed in 207 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16115540/16115540 [00:27<00:00, 588707.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 2508330\n"
     ]
    }
   ],
   "source": [
    "#get the code and comment snippets from the Extract Python Pairs file\n",
    "code_snippets, comments = EPP.get_python_pairs(EPP.get_all_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ENDMARKER = 0\n",
    "NAME = 1\n",
    "NUMBER = 2\n",
    "STRING = 3\n",
    "NEWLINE = 4\n",
    "INDENT = 5\n",
    "DEDENT = 6\n",
    "LPAR = 7\n",
    "RPAR = 8\n",
    "LSQB = 9\n",
    "RSQB = 10\n",
    "COLON = 11\n",
    "COMMA = 12\n",
    "SEMI = 13\n",
    "PLUS = 14\n",
    "MINUS = 15\n",
    "STAR = 16\n",
    "SLASH = 17\n",
    "VBAR = 18\n",
    "AMPER = 19\n",
    "LESS = 20\n",
    "GREATER = 21\n",
    "EQUAL = 22\n",
    "DOT = 23\n",
    "PERCENT = 24\n",
    "LBRACE = 25\n",
    "RBRACE = 26\n",
    "EQEQUAL = 27\n",
    "NOTEQUAL = 28\n",
    "LESSEQUAL = 29\n",
    "GREATEREQUAL = 30\n",
    "TILDE = 31\n",
    "CIRCUMFLEX = 32\n",
    "LEFTSHIFT = 33\n",
    "RIGHTSHIFT = 34\n",
    "DOUBLESTAR = 35\n",
    "PLUSEQUAL = 36\n",
    "MINEQUAL = 37\n",
    "STAREQUAL = 38\n",
    "SLASHEQUAL = 39\n",
    "PERCENTEQUAL = 40\n",
    "AMPEREQUAL = 41\n",
    "VBAREQUAL = 42\n",
    "CIRCUMFLEXEQUAL = 43\n",
    "LEFTSHIFTEQUAL = 44\n",
    "RIGHTSHIFTEQUAL = 45\n",
    "DOUBLESTAREQUAL = 46\n",
    "DOUBLESLASH = 47\n",
    "DOUBLESLASHEQUAL = 48\n",
    "AT = 49\n",
    "ATEQUAL = 50\n",
    "RARROW = 51\n",
    "ELLIPSIS = 52\n",
    "COLONEQUAL = 53\n",
    "OP = 54\n",
    "AWAIT = 55\n",
    "ASYNC = 56\n",
    "TYPE_IGNORE = 57\n",
    "TYPE_COMMENT = 58\n",
    "# These aren't used by the C tokenizer but are needed for tokenize.py\n",
    "ERRORTOKEN = 59\n",
    "COMMENT = 60\n",
    "NL = 61\n",
    "ENCODING = 62\n",
    "N_TOKENS = 63\n",
    "# Special definitions for cooperation with parser\n",
    "NT_OFFSET = 256\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.python.org/3/library/tokenize.html\n",
    "def tokenize_python(code_snippet):\n",
    "    tokens = tokenize.tokenize(io.BytesIO(code_snippet.encode('utf-8')).readline)\n",
    "    parsed = []\n",
    "    for token in tokens:\n",
    "        if token.type not in [0,59,60,61,62,63,256]:\n",
    "            #print('----')\n",
    "            #print(token)\n",
    "            parsed.append(token.string)\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  ', 'def', '__call__', '(', 'mcs', ',', '*', 'args', ',', '**', 'kwargs', ')', ':', '\\n', '\\n', '    ', 'if', 'kwargs', ':', '\\n', '      ', 'return', 'type', '.', '__call__', '(', 'mcs', ',', '*', 'args', ',', '**', 'kwargs', ')', '\\n', '', 'else', ':', '\\n', '      ', 'instances', '=', 'mcs', '.', '_instances', '\\n', 'key', '=', '(', 'mcs', ',', ')', '+', 'tuple', '(', 'args', ')', '\\n', 'try', ':', '\\n', '        ', 'return', 'instances', '[', 'key', ']', '\\n', '', 'except', 'KeyError', ':', '\\n', '# No cache entry for key exists, create a new one.', '\\n', '        ', 'return', 'instances', '.', 'setdefault', '(', 'key', ',', 'type', '.', '__call__', '(', 'mcs', ',', '*', 'args', ')', ')', '\\n', '', 'except', 'TypeError', ':', '\\n', '# An object in args cannot be hashed, always return', '\\n', '# a new instance.', '\\n', '        ', 'return', 'type', '.', '__call__', '(', 'mcs', ',', '*', 'args', ')', '\\n', '\\n', '\\n', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize_python(code_snippets[0])\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
