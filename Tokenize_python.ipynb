{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Extract_Python_Pairs.ipynb\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import import_ipynb\n",
    "import Extract_Python_Pairs as EPP\n",
    "#https://docs.python.org/3/library/tokenize.html\n",
    "import tokenize\n",
    "import io\n",
    "import numpy as np\n",
    "from tqdm import tqdm #inline progress bar (quality of life)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 58717/16115540 [00:00<00:27, 587128.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed in 179 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16115540/16115540 [00:20<00:00, 798420.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 2508330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###get the code and comment snippets from the Extract Python Pairs file\n",
    "code_snippets, comments = EPP.get_python_pairs(EPP.get_all_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nENDMARKER = 0\\nNAME = 1\\nNUMBER = 2\\nSTRING = 3\\nNEWLINE = 4\\nINDENT = 5\\nDEDENT = 6\\nLPAR = 7\\nRPAR = 8\\nLSQB = 9\\nRSQB = 10\\nCOLON = 11\\nCOMMA = 12\\nSEMI = 13\\nPLUS = 14\\nMINUS = 15\\nSTAR = 16\\nSLASH = 17\\nVBAR = 18\\nAMPER = 19\\nLESS = 20\\nGREATER = 21\\nEQUAL = 22\\nDOT = 23\\nPERCENT = 24\\nLBRACE = 25\\nRBRACE = 26\\nEQEQUAL = 27\\nNOTEQUAL = 28\\nLESSEQUAL = 29\\nGREATEREQUAL = 30\\nTILDE = 31\\nCIRCUMFLEX = 32\\nLEFTSHIFT = 33\\nRIGHTSHIFT = 34\\nDOUBLESTAR = 35\\nPLUSEQUAL = 36\\nMINEQUAL = 37\\nSTAREQUAL = 38\\nSLASHEQUAL = 39\\nPERCENTEQUAL = 40\\nAMPEREQUAL = 41\\nVBAREQUAL = 42\\nCIRCUMFLEXEQUAL = 43\\nLEFTSHIFTEQUAL = 44\\nRIGHTSHIFTEQUAL = 45\\nDOUBLESTAREQUAL = 46\\nDOUBLESLASH = 47\\nDOUBLESLASHEQUAL = 48\\nAT = 49\\nATEQUAL = 50\\nRARROW = 51\\nELLIPSIS = 52\\nCOLONEQUAL = 53\\nOP = 54\\nAWAIT = 55\\nASYNC = 56\\nTYPE_IGNORE = 57\\nTYPE_COMMENT = 58\\n# These aren't used by the C tokenizer but are needed for tokenize.py\\nERRORTOKEN = 59\\nCOMMENT = 60\\nNL = 61\\nENCODING = 62\\nN_TOKENS = 63\\n# Special definitions for cooperation with parser\\nNT_OFFSET = 256\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ENDMARKER = 0\n",
    "NAME = 1\n",
    "NUMBER = 2\n",
    "STRING = 3\n",
    "NEWLINE = 4\n",
    "INDENT = 5\n",
    "DEDENT = 6\n",
    "LPAR = 7\n",
    "RPAR = 8\n",
    "LSQB = 9\n",
    "RSQB = 10\n",
    "COLON = 11\n",
    "COMMA = 12\n",
    "SEMI = 13\n",
    "PLUS = 14\n",
    "MINUS = 15\n",
    "STAR = 16\n",
    "SLASH = 17\n",
    "VBAR = 18\n",
    "AMPER = 19\n",
    "LESS = 20\n",
    "GREATER = 21\n",
    "EQUAL = 22\n",
    "DOT = 23\n",
    "PERCENT = 24\n",
    "LBRACE = 25\n",
    "RBRACE = 26\n",
    "EQEQUAL = 27\n",
    "NOTEQUAL = 28\n",
    "LESSEQUAL = 29\n",
    "GREATEREQUAL = 30\n",
    "TILDE = 31\n",
    "CIRCUMFLEX = 32\n",
    "LEFTSHIFT = 33\n",
    "RIGHTSHIFT = 34\n",
    "DOUBLESTAR = 35\n",
    "PLUSEQUAL = 36\n",
    "MINEQUAL = 37\n",
    "STAREQUAL = 38\n",
    "SLASHEQUAL = 39\n",
    "PERCENTEQUAL = 40\n",
    "AMPEREQUAL = 41\n",
    "VBAREQUAL = 42\n",
    "CIRCUMFLEXEQUAL = 43\n",
    "LEFTSHIFTEQUAL = 44\n",
    "RIGHTSHIFTEQUAL = 45\n",
    "DOUBLESTAREQUAL = 46\n",
    "DOUBLESLASH = 47\n",
    "DOUBLESLASHEQUAL = 48\n",
    "AT = 49\n",
    "ATEQUAL = 50\n",
    "RARROW = 51\n",
    "ELLIPSIS = 52\n",
    "COLONEQUAL = 53\n",
    "OP = 54\n",
    "AWAIT = 55\n",
    "ASYNC = 56\n",
    "TYPE_IGNORE = 57\n",
    "TYPE_COMMENT = 58\n",
    "# These aren't used by the C tokenizer but are needed for tokenize.py\n",
    "ERRORTOKEN = 59\n",
    "COMMENT = 60\n",
    "NL = 61\n",
    "ENCODING = 62\n",
    "N_TOKENS = 63\n",
    "# Special definitions for cooperation with parser\n",
    "NT_OFFSET = 256\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "  def _create_file_if_needed(self):\n",
      "    \n",
      "    if not os.path.exists(self._file.filename()):\n",
      "      old_umask = os.umask(0177)\n",
      "      try:\n",
      "        open(self._file.filename(), 'a+b').close()\n",
      "      finally:\n",
      "        os.umask(old_umask)\n",
      "\n",
      "\n",
      "230\n",
      "\n",
      "Tokenized:\n",
      "['<start>', '  ', 'def', '_create_file_if_needed', '(', 'self', ')', ':', '\\n', '    ', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'self', '.', '_file', '.', 'filename', '(', ')', ')', ':', '\\n', '      ', 'old_umask', '=', 'os', '.', 'umask', '(', '<number>', '<number>', ')', '\\n', 'try', ':', '\\n', '        ', 'open', '(', 'self', '.', '_file', '.', 'filename', '(', ')', ',', '<string>', ')', '.', 'close', '(', ')', '\\n', '', 'finally', ':', '\\n', '        ', 'os', '.', 'umask', '(', 'old_umask', ')', '\\n', '', '', '', '', '<end>']\n",
      "number of tokens: 77\n"
     ]
    }
   ],
   "source": [
    "#https://docs.python.org/3/library/tokenize.html\n",
    "def tokenize_python(code_snippet):\n",
    "    tokens = tokenize.tokenize(io.BytesIO(code_snippet.encode('utf-8')).readline)\n",
    "    parsed = []\n",
    "    parsed.append('<start>')\n",
    "    for token in tokens:\n",
    "        if token.type not in [0,57,58,59,60,61,62,63,256]:\n",
    "            #print('----')\n",
    "            #print(token.type, token.string)\n",
    "            #print(token)\n",
    "            \n",
    "            #string\n",
    "            if token.type == 2:\n",
    "                parsed.append('<number>')\n",
    "            elif token.type == 3:\n",
    "                parsed.append('<string>')\n",
    "            else:\n",
    "                parsed.append(token.string)\n",
    "                \n",
    "                \n",
    "    parsed.append('<end>')\n",
    "    return parsed\n",
    "\n",
    "#EXAMPLE\n",
    "num = 3\n",
    "#'''\n",
    "print('Original:')\n",
    "print(code_snippets[num])\n",
    "print(len(code_snippets[num]))\n",
    "print()\n",
    "print('Tokenized:')\n",
    "print(tokenize_python(code_snippets[num]))\n",
    "print('number of tokens:',len(tokenize_python(code_snippets[num])))\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small_code = code_snippets[:10000]\n",
    "#small_comment =  comments[:10000]\n",
    "small_code = code_snippets\n",
    "small_comment =  comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2508330/2508330 [16:20<00:00, 2557.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of errors: 74483 or approixmatiely  2.9694258729911933 % of samples\n"
     ]
    }
   ],
   "source": [
    "def get_valid_pairs_and_vocabs(small_comment,small_code):\n",
    "\n",
    "\n",
    "    Source_Sequences = [] #eng_sent = []\n",
    "    Target_Sequences = [] #fra_sent = []\n",
    "    Source_Tokens = set() #eng_chars = set()\n",
    "    Target_Tokens = set() #fra_chars = set()\n",
    "\n",
    "\n",
    "    errors = 0\n",
    "    for i in tqdm(range(len(small_code))):\n",
    "        try:\n",
    "            Eng = ''.join(ch for ch in small_comment[i] if ch.isalnum() or ch ==' ').lower()\n",
    "            Eng_toks = nltk.word_tokenize(Eng)\n",
    "            Target_Sequences.append(Eng_toks)\n",
    "\n",
    "            python_toks = tokenize_python(small_code[i])\n",
    "            Source_Sequences.append(python_toks)\n",
    "\n",
    "\n",
    "            for nodeType in python_toks:\n",
    "                if (nodeType not in Source_Tokens):\n",
    "                    Source_Tokens.add(nodeType)\n",
    "\n",
    "\n",
    "            for word in Eng_toks:\n",
    "                if (word not in Target_Tokens):\n",
    "                    Target_Tokens.add(word)\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            errors+=1\n",
    "\n",
    "    print('number of errors:',errors,'or approixmatiely ',(errors/len(small_code))*100,'% of samples')\n",
    "    return Source_Sequences,Target_Sequences, Source_Tokens, Target_Tokens\n",
    "Source_Sequences,Target_Sequences, Source_Tokens, Target_Tokens = get_valid_pairs_and_vocabs(small_comment,small_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length source:  638\n",
      "source vocab:  2698909\n",
      "max length target:  190\n",
      "target vocab:  1655801\n"
     ]
    }
   ],
   "source": [
    "Source_Tokens = sorted(list(Source_Tokens))\n",
    "Target_Tokens = sorted(list(Target_Tokens))\n",
    "nb_samples= len(Source_Sequences) #for all\n",
    "\n",
    "# dictionary to index each english character - key is index and value is english character\n",
    "source_index_to_token_dict = {}\n",
    "\n",
    "# dictionary to get english character given its index - key is english character and value is index\n",
    "source_token_to_index_dict = {}\n",
    "\n",
    "for k, v in enumerate(Source_Tokens):\n",
    "    source_index_to_token_dict[k] = v\n",
    "    source_token_to_index_dict[v] = k\n",
    "\n",
    "# dictionary to index each french character - key is index and value is french character\n",
    "target_index_to_token_dict = {}\n",
    "\n",
    "# dictionary to get french character given its index - key is french character and value is index\n",
    "target_token_to_index_dict = {}\n",
    "\n",
    "for k, v in enumerate(Target_Tokens):\n",
    "    target_index_to_token_dict[k] = v\n",
    "    target_token_to_index_dict[v] = k\n",
    "    \n",
    "    \n",
    "max_len_source_seq = max([len(line) for line in Source_Sequences])\n",
    "max_len_target_seq = max([len(line) for line in Target_Sequences])\n",
    "\n",
    "print('max length source: ',max_len_source_seq)\n",
    "print('source vocab: ',len(Source_Tokens))\n",
    "print('max length target: ',max_len_target_seq)\n",
    "print('target vocab: ',len(Target_Tokens))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
