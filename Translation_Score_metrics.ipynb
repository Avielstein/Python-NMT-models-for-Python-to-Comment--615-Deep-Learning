{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Various NLTK Translation Metrics\n",
    "##https://www.nltk.org/api/nltk.translate.html\n",
    "#Meteor for example\n",
    "#https://www.nltk.org/_modules/nltk/translate/meteor_score.html\n",
    "#how to calcualte the blue score\n",
    "#https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "#vaious loss functions\n",
    "##https://towardsdatascience.com/creating-custom-loss-functions-using-tensorflow-2-96c123d5ce6c\n",
    "#custom calback\n",
    "#https://www.tensorflow.org/guide/keras/custom_callback\n",
    "#Smoothing function\n",
    "#https://stackoverflow.com/questions/46444656/bleu-scores-could-i-use-nltk-translate-bleu-score-sentence-bleu-for-calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/avielstein/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu \n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.chrf_score import sentence_chrf\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "from nltk.translate.nist_score import sentence_nist\n",
    "from nltk.translate.ribes_score import sentence_ribes\n",
    "import nltk\n",
    "nltk.download('wordnet') #reqired for meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(reference, hypothesis):\n",
    "    #BLEU\n",
    "    #references -- (list(list(str))) – reference sentences\n",
    "    #hypothesis (list(str)) – a hypothesis sentence\n",
    "    try:\n",
    "        #several methods of smoothing can be used, 1-7. They are good for handleing short sentences\n",
    "        score = sentence_bleu(reference, hypothesis, smoothing_function=SmoothingFunction().method1)\n",
    "        print('BLEU score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with BLEU:',e)\n",
    "        \n",
    "\n",
    "    #CHRF\n",
    "    #references (list(str) / str) – reference sentence\n",
    "    #hypothesis (list(str) / str) – a hypothesis sentence\n",
    "    chrf_reference = reference[0]\n",
    "    chrf_hypothesis = hypothesis\n",
    "    try:\n",
    "        score = sentence_chrf(chrf_reference, chrf_hypothesis)\n",
    "        print('CHRF score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with CHRF:',e)\n",
    "\n",
    "    #GLUE\n",
    "    #references (list(list(str))) – a list of reference sentences\n",
    "    #hypothesis (list(str)) – a hypothesis sentence\n",
    "    try:\n",
    "        score = sentence_gleu(reference, hypothesis)\n",
    "        print('GLEU score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with GLEU:',e)\n",
    "\n",
    "    #METEOR\n",
    "    #references (list(str)) – reference sentences \n",
    "    #hypothesis (str) – a hypothesis sentence\n",
    "    meteor_reference = ' '.join(reference[0])\n",
    "    meteor_hypothesis = ' '.join(hypothesis)\n",
    "    try:\n",
    "        score = single_meteor_score(meteor_reference, meteor_hypothesis)\n",
    "        print('METEOR score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with METEOR:',e)\n",
    "\n",
    "    #NIST\n",
    "    #references (list(list(str))) – reference sentences\n",
    "    #hypothesis (list(str)) – a hypothesis sentence\n",
    "    try:\n",
    "        score = sentence_nist(reference, hypothesis)\n",
    "        print('NIST score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with NIST:',e)\n",
    "\n",
    "    #RIBES\n",
    "    #references list(list(str))) - a list of reference sentences\n",
    "    #hypothesis (list(str)) – a hypothesis sentence\n",
    "    try:\n",
    "        score = sentence_ribes(reference, hypothesis)\n",
    "        print('RIBES score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with RIBES:',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "BLEU score: 0.668740304976422\n",
      "CHRF score: 0.7079531579531578\n",
      "GLEU score: 0.7142857142857143\n",
      "METEOR score: 0.7937500000000002\n",
      "NIST score: 1.8575424759098897\n",
      "RIBES score: 0.9457416090031758\n",
      "------------------------------------------------\n",
      "BLEU score: 0.3327714551776235\n",
      "CHRF score: 0.48218860049810636\n",
      "GLEU score: 0.5\n",
      "METEOR score: 0.7653061224489797\n",
      "-!- Issue with NIST: division by zero\n",
      "RIBES score: 0.4876549560141663\n",
      "------------------------------------------------\n",
      "BLEU score: 0.28574404296988\n",
      "CHRF score: 0.4607912462952685\n",
      "GLEU score: 0.5\n",
      "METEOR score: 0.996\n",
      "NIST score: 1.8575424759098897\n",
      "RIBES score: 0.4728708045015879\n",
      "------------------------------------------------\n",
      "BLEU score: 0.08848885323457842\n",
      "CHRF score: 0.39603981587896636\n",
      "GLEU score: 0.2857142857142857\n",
      "METEOR score: 0.40816326530612246\n",
      "-!- Issue with NIST: division by zero\n",
      "RIBES score: 0.0\n",
      "------------------------------------------------\n",
      "BLEU score: 0.2295748846661433\n",
      "CHRF score: 0.5739518477234905\n",
      "GLEU score: 0.4444444444444444\n",
      "METEOR score: 0.8745098039215687\n",
      "NIST score: 1.9349400790728017\n",
      "RIBES score: 0.28663283766131004\n"
     ]
    }
   ],
   "source": [
    "reference = [['this', 'is', 'a', 'hard','test']]\n",
    "hypotheses = [['that', 'is', 'a','hard', 'test'],\n",
    "              ['this', 'is', 'a', 'test'],\n",
    "              ['this', 'is', 'a','difficult', 'test'],\n",
    "              ['this', 'test', 'is','hard'],\n",
    "              ['this', 'test', 'is','a','hard','one']]\n",
    "\n",
    "\n",
    "\n",
    "for hypothesis in hypotheses:\n",
    "    print('------------------------------------------------')\n",
    "    get_scores(reference,hypothesis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SmoothingFunction.method4 of <nltk.translate.bleu_score.SmoothingFunction object at 0x7fbd3004f828>>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
