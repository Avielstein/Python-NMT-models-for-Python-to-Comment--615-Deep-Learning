{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Various NLTK Translation Metrics\n",
    "##https://www.nltk.org/api/nltk.translate.html\n",
    "#Meteor for example\n",
    "#https://www.nltk.org/_modules/nltk/translate/meteor_score.html\n",
    "#how to calcualte the blue score\n",
    "#https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "#vaious loss functions\n",
    "##https://towardsdatascience.com/creating-custom-loss-functions-using-tensorflow-2-96c123d5ce6c\n",
    "#custom calback\n",
    "#https://www.tensorflow.org/guide/keras/custom_callback\n",
    "#Smoothing function\n",
    "#https://stackoverflow.com/questions/46444656/bleu-scores-could-i-use-nltk-translate-bleu-score-sentence-bleu-for-calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/avielstein/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu \n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.chrf_score import sentence_chrf\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "from nltk.translate.nist_score import sentence_nist\n",
    "from nltk.translate.ribes_score import sentence_ribes\n",
    "import nltk\n",
    "nltk.download('wordnet') #reqired for meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_smoothing(reference, hypothesis):\n",
    "    print('Blue with various smoothing functions:')\n",
    "    try:\n",
    "        score = sentence_bleu(reference, hypothesis, smoothing_function=SmoothingFunction().method1)\n",
    "        print('W/ Smoothing Method 1, score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with Method 1:',e)\n",
    "    try:\n",
    "        score = sentence_bleu(reference, hypothesis, smoothing_function=SmoothingFunction().method2)\n",
    "        print('W/ Smoothing Method 2, score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with Method 2:',e)\n",
    "    try:\n",
    "        score = sentence_bleu(reference, hypothesis, smoothing_function=SmoothingFunction().method3)\n",
    "        print('W/ Smoothing Method 3, score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with Method 3:',e)\n",
    "    try:\n",
    "        score = sentence_bleu(reference, hypothesis, smoothing_function=SmoothingFunction().method4)\n",
    "        print('W/ Smoothing Method 4, score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with Method 4:',e)\n",
    "    try:\n",
    "        score = sentence_bleu(reference, hypothesis, smoothing_function=SmoothingFunction().method5)\n",
    "        print('W/ Smoothing Method 5, score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with Method 5:',e)\n",
    "    try:\n",
    "        score = sentence_bleu(reference, hypothesis, smoothing_function=SmoothingFunction().method6)\n",
    "        print('W/ Smoothing Method 6, score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with Method 6:',e)\n",
    "    try:\n",
    "        score = sentence_bleu(reference, hypothesis, smoothing_function=SmoothingFunction().method7)\n",
    "        print('W/ Smoothing Method 7, score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with Method 7:',e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(reference, hypothesis):\n",
    "    #BLEU\n",
    "    #references -- (list(list(str))) – reference sentences\n",
    "    #hypothesis (list(str)) – a hypothesis sentence\n",
    "    try:\n",
    "        #several methods of smoothing can be used, 1-7. They are good for handleing short sentences\n",
    "        score = sentence_bleu(reference, hypothesis, smoothing_function=SmoothingFunction().method1)\n",
    "        print('BLEU (method 4) score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with BLEU:',e)\n",
    "        \n",
    "\n",
    "    #CHRF\n",
    "    #references (list(str) / str) – reference sentence\n",
    "    #hypothesis (list(str) / str) – a hypothesis sentence\n",
    "    chrf_reference = reference[0]\n",
    "    chrf_hypothesis = hypothesis\n",
    "    try:\n",
    "        score = sentence_chrf(chrf_reference, chrf_hypothesis)\n",
    "        print('CHRF score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with CHRF:',e)\n",
    "\n",
    "    #GLUE\n",
    "    #references (list(list(str))) – a list of reference sentences\n",
    "    #hypothesis (list(str)) – a hypothesis sentence\n",
    "    try:\n",
    "        score = sentence_gleu(reference, hypothesis)\n",
    "        print('GLEU score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with GLEU:',e)\n",
    "\n",
    "    #METEOR\n",
    "    #references (list(str)) – reference sentences \n",
    "    #hypothesis (str) – a hypothesis sentence\n",
    "    meteor_reference = ' '.join(reference[0])\n",
    "    meteor_hypothesis = ' '.join(hypothesis)\n",
    "    try:\n",
    "        score = single_meteor_score(meteor_reference, meteor_hypothesis)\n",
    "        print('METEOR score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with METEOR:',e)\n",
    "\n",
    "    #NIST\n",
    "    #references (list(list(str))) – reference sentences\n",
    "    #hypothesis (list(str)) – a hypothesis sentence\n",
    "    try:\n",
    "        score = sentence_nist(reference, hypothesis)\n",
    "        print('NIST score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with NIST:',e)\n",
    "\n",
    "    #RIBES\n",
    "    #references (list(list(str))) - a list of reference sentences\n",
    "    #hypothesis (list(str)) – a hypothesis sentence\n",
    "    try:\n",
    "        score = sentence_ribes(reference, hypothesis)\n",
    "        print('RIBES score:',score)\n",
    "    except Exception as e:\n",
    "        print('-!- Issue with RIBES:',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "BLEU (method 4) score: 0.668740304976422\n",
      "CHRF score: 0.7079531579531578\n",
      "GLEU score: 0.7142857142857143\n",
      "METEOR score: 0.7937500000000002\n",
      "NIST score: 1.8575424759098897\n",
      "RIBES score: 0.9457416090031758\n",
      "Blue with various smoothing functions:\n",
      "W/ Smoothing Method 1, score: 0.668740304976422\n",
      "W/ Smoothing Method 2, score: 0.7598356856515925\n",
      "W/ Smoothing Method 3, score: 0.668740304976422\n",
      "W/ Smoothing Method 4, score: 0.668740304976422\n",
      "W/ Smoothing Method 5, score: 0.7046977379006782\n",
      "W/ Smoothing Method 6, score: 0.7045254735893448\n",
      "W/ Smoothing Method 7, score: 0.7046977379006782\n",
      "------------------------------------------------\n",
      "BLEU (method 4) score: 0.3327714551776235\n",
      "CHRF score: 0.48218860049810636\n",
      "GLEU score: 0.5\n",
      "METEOR score: 0.7653061224489797\n",
      "-!- Issue with NIST: division by zero\n",
      "RIBES score: 0.4876549560141663\n",
      "Blue with various smoothing functions:\n",
      "W/ Smoothing Method 1, score: 0.3327714551776235\n",
      "W/ Smoothing Method 2, score: 0.5506953149031838\n",
      "W/ Smoothing Method 3, score: 0.49760938992507125\n",
      "W/ Smoothing Method 4, score: 0.9487304297576223\n",
      "W/ Smoothing Method 5, score: 0.3863614581487129\n",
      "W/ Smoothing Method 6, score: 0.4158379817615003\n",
      "W/ Smoothing Method 7, score: 1.3055729396584415\n",
      "------------------------------------------------\n",
      "BLEU (method 4) score: 0.28574404296988\n",
      "CHRF score: 0.4607912462952685\n",
      "GLEU score: 0.5\n",
      "METEOR score: 0.996\n",
      "NIST score: 1.8575424759098897\n",
      "RIBES score: 0.4728708045015879\n",
      "Blue with various smoothing functions:\n",
      "W/ Smoothing Method 1, score: 0.28574404296988\n",
      "W/ Smoothing Method 2, score: 0.537284965911771\n",
      "W/ Smoothing Method 3, score: 0.42728700639623407\n",
      "W/ Smoothing Method 4, score: 0.7987822049880149\n",
      "W/ Smoothing Method 5, score: 0.3840020289315366\n",
      "W/ Smoothing Method 6, score: 0.3701867781534458\n",
      "W/ Smoothing Method 7, score: 1.0588856840589143\n",
      "------------------------------------------------\n",
      "BLEU (method 4) score: 0.08848885323457842\n",
      "CHRF score: 0.39603981587896636\n",
      "GLEU score: 0.2857142857142857\n",
      "METEOR score: 0.40816326530612246\n",
      "-!- Issue with NIST: division by zero\n",
      "RIBES score: 0.0\n",
      "Blue with various smoothing functions:\n",
      "W/ Smoothing Method 1, score: 0.08848885323457842\n",
      "W/ Smoothing Method 2, score: 0.3518629739981188\n",
      "W/ Smoothing Method 3, score: 0.1759314869990594\n",
      "W/ Smoothing Method 4, score: 1.7985033798826104\n",
      "W/ Smoothing Method 5, score: 0.149880280583789\n",
      "-!- Issue with Method 6: This smoothing method requires non-zero precision for bigrams.\n",
      "W/ Smoothing Method 7, score: 1.9422548135921596\n",
      "------------------------------------------------\n",
      "BLEU (method 4) score: 0.2295748846661433\n",
      "CHRF score: 0.5739518477234905\n",
      "GLEU score: 0.4444444444444444\n",
      "METEOR score: 0.8745098039215687\n",
      "NIST score: 1.9349400790728017\n",
      "RIBES score: 0.28663283766131004\n",
      "Blue with various smoothing functions:\n",
      "W/ Smoothing Method 1, score: 0.2295748846661433\n",
      "W/ Smoothing Method 2, score: 0.45499414040480374\n",
      "W/ Smoothing Method 3, score: 0.34329452398451965\n",
      "W/ Smoothing Method 4, score: 0.6332925623836378\n",
      "W/ Smoothing Method 5, score: 0.3424797809899507\n",
      "W/ Smoothing Method 6, score: 0.27082257573988505\n",
      "W/ Smoothing Method 7, score: 0.8378077772445672\n"
     ]
    }
   ],
   "source": [
    "reference = [['this', 'is', 'a', 'hard','test']]\n",
    "hypotheses = [['that', 'is', 'a','hard', 'test'],\n",
    "              ['this', 'is', 'a', 'test'],\n",
    "              ['this', 'is', 'a','difficult', 'test'],\n",
    "              ['this', 'test', 'is','hard'],\n",
    "              ['this', 'test', 'is','a','hard','one']]\n",
    "\n",
    "\n",
    "\n",
    "for hypothesis in hypotheses:\n",
    "    print('------------------------------------------------')\n",
    "    get_scores(reference,hypothesis)\n",
    "    compare_smoothing(reference,hypothesis)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
